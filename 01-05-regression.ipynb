{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fn(f, *params):\n",
    "    plt.scatter(X, Y)\n",
    "    x = np.linspace(min(X), max(X), 100)\n",
    "    y = [f(*params, x_) for x_ in x]\n",
    "    plt.plot(x, y, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (-0.8, 30.5), (3.8, 40.6), (7.6, 51.2), (10.8, 49.9), (15.8, 63.4),\n",
    "    (15.9, 57.8), (20.4, 80.6), (24.1, 73.9), (25.5, 72.5), (32.5, 86.8),\n",
    "    (35.0, 100.1), (36.6, 93.2), (40.2, 101.3), (44.1, 109.4), (48.0, 117.3),\n",
    "    (52.7, 128.2), (55.0, 137.0), (58.6, 140.8), (61.2, 140.8), (66.5, 155.6),\n",
    "    (68.2, 156.3), (71.3, 159.7), (76.6, 169.6), (79.0, 174.0), (84.5, 176.4),\n",
    "    (83.7, 183.4), (90.5, 200.1), (92.9, 199.2), (97.6, 200.3), (100.3, 217.9)]\n",
    "\n",
    "X, Y = list(zip(*data))\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the search space and the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume that there's a linear relationship between $x$ and $y$. Below, $a$ and $b$ are parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a, b, x):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use mean squared error loss:\n",
    "\n",
    "$$ l(a, b) = \\frac{1}{n} \\sum_{i = 1}^{n} \\left( f(a, b, X_i) - Y_i \\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(a, b):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what the loss surface looks like.\n",
    "\n",
    "Eventually, we'll want to find an $a$ and $b$ that minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 300\n",
    "a_space = np.linspace(1, 3, num_samples)\n",
    "b_space = np.linspace(-30, 100, num_samples)\n",
    "losses = np.zeros((num_samples, num_samples))\n",
    "for i in range(len(a_space)):\n",
    "    for j in range(len(b_space)):\n",
    "        losses[j, i] = loss(a_space[i], b_space[j])\n",
    "losses = np.log(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize log-loss since it looks nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.pcolor(a_space, b_space, losses, cmap=cm.RdBu_r, vmin=losses.min(), vmax=losses.max())\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.colorbar(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need $\\frac{\\partial l}{\\partial a}$ and $\\frac{\\partial l}{\\partial b}$ in order to do gradient descent on our parameters. First, let's compute those via the chain rule (01-06-notes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_a(a, b):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_b(a, b):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit partial_a(1, 2), partial_b(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use backpropagation (dynamic programming) to speed this up (01-06-notes). Also, take note of the forward pass (computing outputs of the operations in the graph) and backward pass (computing derivatives of the operations in the graph).\n",
    "\n",
    "The absolute speedup here is small because this computation is so fast, but when you have large and deep neural networks, and you're differentiating with respect to millions of parameters, this speedup is huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a tuple (dl/da, dl/db)\n",
    "def compute_grad(a, b):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit compute_grad(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even in this simple example, we see a speedup. The naive version is about 50% slower than the version using backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_init = 2.0 # a reasonable initial guess\n",
    "b_init = 3.0\n",
    "alpha = 1e-4 # learning rate\n",
    "\n",
    "a = a_init\n",
    "b = b_init\n",
    "\n",
    "losses = []\n",
    "for iteration in range(100000):\n",
    "    # compute loss (for visualization)\n",
    "    losses.append(loss(a, b))\n",
    "    \n",
    "    # compute gradients\n",
    "    # TODO\n",
    "    \n",
    "    # update parameters\n",
    "    # TODO\n",
    "\n",
    "print('final loss=%f' % loss(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how our loss changed over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylim(0, np.max(losses))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our learned function, along with our data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fn(f, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is our learned $f$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('f(x) = %.1fx + %.1f' % (a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've been doing has been pretty mechanical, and it can be automated. TensorFlow is a library for writing computation graphs and automatically differentiating through them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta = \\begin{bmatrix}\n",
    "a \\\\\n",
    "b\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.Variable([a_init, b_init])\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the function (search space) and the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None])\n",
    "y = tf.placeholder(tf.float32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_tf = theta[0] * x + theta[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tf = tf.reduce_sum((f_tf - y)**2) / len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(a_init, b_init) # numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tf.eval({x: X, y: Y}) # from tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically compute the gradient!\n",
    "\n",
    "TensorFlow supports automatic differentiation, so this is all we need to do in order to compute the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_theta, = tf.gradients(loss_tf, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_theta = theta - alpha * d_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "We're actually not going to use our `d_theta` or `next_theta`. We're going to use one of TensorFlow's built-in optimizers, which will even take the step of calling `tf.gradients` for us.\n",
    "\n",
    "Let's use a fancier variation of gradient descent to optimize this: [Adam](https://arxiv.org/abs/1412.6980) (Kingma & Ba 2014). We can instantiate TensorFlow's Adam optimizer and tell it what quantity we want to minimize. It'll figure out the rest (including automatically computing the gradient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "sess.run(tf.global_variables_initializer()) # initialize theta (and also Adam internal state variables)\n",
    "for i in range(5000):\n",
    "    l, _ = sess.run([loss_tf, optim], {x: X, y: Y})\n",
    "    losses.append(l)\n",
    "    \n",
    "print('final loss=%f' % loss_tf.eval({x: X, y: Y}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.ylim(0, np.max(losses))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the learned parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the learned function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fn(f, *theta.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimizing something else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use gradient descent to minimize any quantity. For example, if we define loss as $|f(x) - x|^2$, we can optimize over values of $x$ to find the fixed point of the function (the point where $f(x) = x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of using the line of best fit, which is not exactly correct due to noise in the data,\n",
    "# let's reset this back to the Celsius to Fahrenheit function\n",
    "sess.run(theta.assign([1.8, 32]))\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fp = tf.square(f_tf - x)\n",
    "g_loss_fp, = tf.gradients(loss_fp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "\n",
    "fixed_point = np.random.normal(size=(1,)) # initial guess\n",
    "\n",
    "for i in range(10):\n",
    "    g, l = sess.run([g_loss_fp, loss_fp], {x: fixed_point})\n",
    "    fixed_point = fixed_point - alpha * g_loss_fp.eval({x: fixed_point})\n",
    "\n",
    "print('fixed point: f(%.1f) = %.1f' % (fixed_point, f_tf.eval({x: fixed_point})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
